<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Pitch Deck">
  <meta name="keywords" content="Pitch Deck">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multimodal Foundation Modeling</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body" style="padding-bottom: 0rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Pitch Deck</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://wx83.github.io/">Weihan Xu</a>,
              <a href="https://iftrush.github.io/">Kan Jen Cheng</a>,
              <a href="https://rayxsong.com/">Ray Song</a>
            </span>
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">EECS, UC Berkeley</span>

          </div> -->


            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Featured Video Section -->
<section class="hero is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Our Vision</h2>
          <div class="video-embed" style="margin-top: 1.5rem;">
            <iframe
              src="https://www.youtube.com/embed/jxdmsv1229Y"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              allowfullscreen
              style="width: 100%; aspect-ratio: 16 / 9; border: 0;">
            </iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Our Mission</h2>
        <div class="content has-text-justified">
          <p style="font-size: 1.1rem; line-height: 1.7;">
            <strong>Our Mission:</strong> Build a human-like multimodal system spanning vision, audio, touch, taste, and smell with joint perception, reusable memory, and coherent control—so it can align signals in parallel, reason over long horizons, and act consistently in real-world, interactive settings.
          </p>
          
          <div style="margin-top: 2rem;">
            <h3 class="title is-4">Joint Perception (audio–video foundation + tactile moat)</h3>
            <p style="margin-bottom: 1.5rem;">
              For audio and video, we've collected a large-scale corpus of long-form, in-the-wild videos with aligned audio and metadata, purpose-built to support multimodal planning, retrieval, and generation. Beyond audio and video, we have something uniquely defensible: a large-scale, curated dataset that synchronizes egocentric video, full-hand tactile force maps, and accurate hand pose across diverse real-world environments—something no existing public or commercial dataset provides. This is supported by a reproducible hardware and data-collection pipeline, including a scalable full-hand tactile glove and robust cross-device synchronization. On top of the data, we have benchmarks, evaluation protocols, and trained multimodal representations that demonstrate how touch grounds perception and action.
            </p>

            <h3 class="title is-4">Reusable Memory (scalable experience library)</h3>
            <p style="margin-bottom: 1.5rem;">
              Our NeurIPS-accepted work shows how to synthesize high-quality short-form outputs directly from raw, real-world video clips. We treat these clips as reusable memory—a growing library of grounded experiences the system can reference, recombine, and adapt to new contexts at scale.
            </p>

            <h3 class="title is-4">Coherent Control (unified multimodal editing)</h3>
            <p style="margin-bottom: 1.5rem;">
              We developed a proprietary architecture for simultaneous audio-video editing, treating modalities as one coupled stream rather than separate tracks. This enables controllable, consistent edits where changes in motion, events, or structure stay temporally and semantically aligned across modalities.
            </p>

            <h3 class="title is-4">Spatial Presence (3D perception and generation, in progress)</h3>
            <p style="margin-bottom: 1.5rem;">
              We are developing models on advanced signals like binaural/spatial audio, enabling physically grounded 3D soundscapes. This strengthens the system's sense of "where" events occur and supports richer world modeling beyond flat 2D content.
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>







<section class="section">
  <div class="container is-max-desktop">
    <!-- Related Links and Works. -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Joint Perception for Audio-Video Learning</h2>
        <div class="content has-text-justified">
          <p>
            We curate paired, long to short multimodal data for film, with clean, time-aligned tracks for dialogue, music, and visuals. These pairs enable high-fidelity generation and distillation from long footage into polished short-form assets while preserving temporal synchronization and providing supervision to learn temporal correspondence in film settings.
          </p>
          <style>
            .assets { width: 100%; max-width: 720px; border-collapse: collapse; font: 14px/1.4 system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
            .assets caption { text-align: left; font-weight: 600; margin-bottom: .5rem; }
            .assets th, .assets td { padding: .6rem .8rem; border-bottom: 1px solid #e5e7eb; vertical-align: top; }
            .assets th { width: 220px; text-align: left; color: #374151; }
            .assets a { word-break: break-all; text-decoration: none; }
            .assets a:hover { text-decoration: underline; }
            .video-embed { margin-top: .5rem; }
            .video-embed iframe { width: 100%; aspect-ratio: 16 / 9; border: 0; }
          </style>

          <table class="assets">
            <tr>
              <th>Input Video</th>
              <td>
                <a href="https://www.youtube.com/embed/WfXRTZdyIe0?start=97" target="_blank" rel="noopener">
                  
                </a>
                <!-- Optional inline preview; delete this block if you don't want it -->
                <div class="video-embed">
                  <iframe
                    src="https://www.youtube.com/embed/WfXRTZdyIe0?start=97"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    allowfullscreen>
                  </iframe>
                </div>
              </td>
            </tr>
            <tr>
              <th>Music Track</th>
              <td>
                <audio controls="">
                  <source src="./static/audio/music_intro.wav" type="audio/wav">
                </audio>
              </td>
            </tr>
            <tr>
              <th>Dialogue Track</th>
              <td>
                <audio controls="">
                  <source src="./static/audio/dialog_intro.wav" type="audio/wav">
                </audio>
              </td>
            </tr>
            <tr>
              <th>Sound Effect Track</th>
              <td>
                <audio controls="">
                  <source src="./static/audio/effect_intro.wav" type="audio/wav">
                </audio>
              </td>
            </tr>
          </table>

          <!-- Joint Perception Demo -->
          <h3 class="title is-4">Joint Perception Demo: Teaser Generation</h3>
          <p style="margin-bottom: 1rem;">
            We run joint perception on audiovisual learning through a downstream task: teaser generation.
          </p>
          <table class="assets">
            <tr>
              <th>Input Video</th>
              <td>
                <a href="https://www.youtube.com/embed/89xTTczbv0E?start=120" target="_blank" rel="noopener">
                  
                </a>
                <!-- Optional inline preview; delete this block if you don't want it -->
                <div class="video-embed">
                  <iframe
                    src="https://www.youtube.com/embed/89xTTczbv0E?start=120"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    allowfullscreen>
                  </iframe>
                </div>
              </td>
            </tr>
          </table>
        
          <div class="columns is-centered">
            <!-- Visual Effects. -->
            <div class="column">
              <div class="content">
                <video id="" controls="" muted="" loop="" playsinline="" height="100%">
                  <source src="./static/video/teasergen-pt.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!--/ Visual Effects. -->
            <div class="column">
              <div class="content">
                <video id="" controls="" muted="" loop="" playsinline="" height="100%">
                  <source src="./static/video/teagergen-lr.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
    <!--/ Related Links and Works. -->
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Reusable Memory (Scalable Experience Library)</h2>
        <div class="content has-text-justified">
          <p>
            In this example, we show how we can retrieve memory from existing footage. With our multimodal foundation model, we can retrieve memory in different forms, such as video, audio, and more.
          </p>

          <!-- Downstream Task Demo -->
          <h3 class="title is-4">Short-Form Video Generation with Grounded Memory</h3>
          <p style="margin-bottom: 1rem;">
            Our model retrieves memory from existing footage and incorporates it into the generation process to create coherent short-form content.
          </p>
          <p style="margin-bottom: 1rem;">
            We run joint perception on audiovisual learning through a downstream task: teaser generation.
          </p>
          <table class="assets">
            <tr>
              <th>Input Video</th>
              <td>
                <a href="https://www.youtube.com/embed/agij_IxGjCI?start=107" target="_blank" rel="noopener">
                  
                </a>
                <!-- Optional inline preview; delete this block if you don't want it -->
                <div class="video-embed">
                  <iframe
                    src="https://www.youtube.com/embed/agij_IxGjCI?start=107"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                    allowfullscreen>
                  </iframe>
                </div>
              </td>
            </tr>
          </table>
        
          <div class="columns is-centered">
            <!-- Visual Effects. -->
            <div class="column">
              <div class="content">
                <video id="" controls="" muted="" loop="" playsinline="" height="100%">
                  <source src="./static/video/regen_idq_tv.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <!--/ Visual Effects. -->
            <div class="column">
              <div class="content">
                <video id="" controls="" muted="" loop="" playsinline="" height="100%">
                  <source src="./static/video/regen_idq_t.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
    <!--/ Related Links and Works. -->
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <!-- Related Links and Works. -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Coherent Control (Unified Multimodal Editing)</h2>
        <div class="content has-text-justified">
          <p>
            Our foundation model can support multimodal input and multimodal output to have a better understanding of your contents and facilitate iterative editing to create new pairs to train more creative and personalized tasks.
          </p>

          <!-- Audiovisual Highlight Section -->
          <h3 class="title is-4">Audiovisual Highlight</h3>
          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Without Highlight</h4>
                <video controls="" muted="" loop="" playsinline="" height="100%">
                  <source src="./static/video/board_input.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <div class="column">
              <div class="content">
                <h4 class="title is-5">With Highlight</h4>
                <video controls="" muted="" loop="" playsinline="" height="100%">
                  <source src="./static/video/board_output_with_audio.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>

          <!-- Audiovisual Inpaint Section -->
          <h3 class="title is-4">Audiovisual Addition</h3>
          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <h4 class="title is-5">Without Seagull</h4>
                <video controls="" muted="" loop="" playsinline="" height="100%">
                  <source src="./static/video/waves_no_seagulls_video.mp4" type="video/mp4">
                </video>
              </div>
            </div>
            <div class="column">
              <div class="content">
                <h4 class="title is-5">With Seagull</h4>
                <video controls="" muted="" loop="" playsinline="" height="100%">
                  <source src="./static/video/waves_with_seagull.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Related Links and Works. -->
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Spatial Presence (3D Perception and Generation)</h2>
        <div class="content has-text-justified">
          <p>
            We are developing models on advanced signals like binaural/spatial audio, enabling physically grounded 3D soundscapes. This strengthens the system's sense of "where" events occur and supports richer world modeling beyond flat 2D content.
          </p>

          <!-- 2D 360 Video to Spatial Audio Demo -->
          <h3 class="title is-4">2D 360 Video to Spatial Audio</h3>
          <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <video controls="" muted="" loop="" playsinline="" height="100%">
                  <source src="./static/video/0FB9jMXMP8A_23.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- New Downstream Task Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Downstream Task: Immersive Streaming Experience Platform</h2>

        <div class="content has-text-justified">
          <p>
            Our downstream task showcases an immersive streaming platform that converts ordinary video into real-time multimodal experiences. The system synchronizes visual understanding, spatial audio, and haptic feedback so users can move beyond passive watching and experience content as interactive, responsive media.
          </p>
          <p>
            As scenes evolve, the model captures motion, impact, rhythm, and environmental transitions, then maps them to tightly aligned feedback channels. By jointly modeling video, audio, and tactile signals over time, the platform maintains temporal coherence and delivers interactions that feel consistent and natural.
          </p>
          <p>
            This demo highlights the core capability of our multimodal foundation model: bridging perception and action. It interprets live or streaming content, generates synchronized multimodal outputs, and adapts feedback behavior based on user context and device constraints.
          </p>

          <h3 class="title is-4">Live Demo</h3>
          <p><strong>Before:</strong> <code>https://youtu.be/JWUfzduW4ss</code></p>
          <p><strong>After:</strong> <code>https://youtu.be/jxdmsv1229Y</code></p>

          <div class="columns is-centered">
            <div class="column">
              <div class="video-container">
                <iframe
                  src="https://www.youtube.com/embed/jxdmsv1229Y"
                  title="Immersive Streaming Demo"
                  frameborder="0"
                  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                  allowfullscreen>
                </iframe>
              </div>
            </div>
          </div>
          <!-- /Demo -->
        </div>
      </div>
    </div>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2508.17623">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Berkeley-Speech-Group/emo-reasoning" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. You are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
